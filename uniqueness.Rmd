---
title: "Calculating uniqueness"
author: "Andrea Ballatore"
date: "27/03/2022"
output: 
  html_document: 
    toc: yes
    self_contained: true
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse, moments, knitr, corrr, entropy, pastecs, coop, sf, 
               outliers, mvoutlier, flextable, rstatix, FSA) # mahalanobis

#' @return x rescaled between 0 and 1
range01 <- function(x){
  den = (max(x,na.rm = T)-min(x,na.rm = T))
  stopifnot(den != 0)
  (x-min(x,na.rm = T))/den
}

#' @return vector of categorical data from percentages and labels
generate_cat_data_from_pc = function(pcs, labs, size){
  stopifnot(length(pcs)==length(labs))
  x = c()
  for (i in 1:length(pcs)){
    pc = pcs[i]
    lab = labs[i]
    n = size * pc/100
    x = c(x, rep(lab,n))
  }
  sample(x)
}

#' rename scale as z score for clarity
zscore <- function(x){scale(x)}

#' @return a tibble with the given value in each cell
gen_const_matrix = function(n_row, n_col, val){
  tib = as_tibble(matrix(nrow=n_row,ncol=n_col))
  tib[,] = val
  return(tib)
}

#' Generate random data for testing.
#' @return a tibble with random values following a given distribution
gen_random_matrix = function(distrib, n_row, n_col, scale=1, group_split=.2){
  #print(paste('gen_random_matrix',distrib, n_row, n_col, scale))
  stopifnot(distrib %in% c('normal','uniform','2groups','3groups'))
  stopifnot(n_row > 0 && n_col > 0)
  stopifnot(group_split > 0 && group_split < 1)
  if (distrib=='normal'){
    rand = as_tibble(matrix(rnorm(n_row*n_col), nrow=n_row))
    return(rand*scale)
  }
  if (distrib=='uniform'){
    return(as_tibble(matrix(runif(n_row*n_col), nrow=n_row))*scale)
  }
  if (distrib=='2groups'){
    # create data with some common and rare groups
    common_tib = gen_random_matrix('normal', n_row * (1-group_split), n_col, scale)
    rare_tib =   gen_random_matrix('normal', n_row * group_split, n_col, scale*1000)
    tib = bind_rows(common_tib, rare_tib)
    return(tib %>% sample_frac(1))
  }
  if (distrib=='3groups'){
    # create data with some common and rare groups
    common_tib = gen_random_matrix('normal', n_row * .7, n_col, scale)
    med_tib =   gen_random_matrix('normal', n_row * .2, n_col, scale*100)
    rare_tib =   gen_random_matrix('normal', n_row * .1, n_col, scale*1000)
    tib = bind_rows(common_tib, med_tib, rare_tib)
    return(tib %>% sample_frac(1))
  }
}


```

# Sandbox

```{r eval=FALSE}

# TODO: handle missing values in dist_uniq with option

# generate matrices
set.seed(50)
unitib = gen_random_matrix('uniform', 1000, 10)
normtib = gen_random_matrix('normal', 8, 3)
gtib = gen_random_matrix('2groups', 20, 3, group_split = .2)
zerotib = gen_const_matrix(8,3,0)
onetib = gen_const_matrix(8,3,1)

gtib = gen_random_matrix('3groups', 100, 5)
ugtib = dist_uniq(gtib, 'euclidean')
desc_stats_uniq(ugtib)

gtib = gen_random_matrix('2groups', 100, 2)
ugtib = dist_uniq(gtib, 'euclidean')
print(desc_stats_uniq(ugtib))

tib = gen_random_matrix('normal', 1000, 5)
ugtib = dist_uniq(tib, 'euclidean')
print(desc_stats_uniq(ugtib))

ugtib = dist_uniq(tib, 'euclidean')
ugtib = dist_uniq(tib, 'mahalanobis')
ugtib = dist_uniq(tib, 'manhattan')
desc_stats_uniq(ugtib)

cov(gen_random_matrix('2groups',100,100))
ugtib = dist_uniq(gen_random_matrix('2groups',100,100), 'mahalanobis')
tib = gen_random_matrix('2groups',100,100)
maha_sum_dists = mahalanobis(tib, colMeans(tib), cov(tib), tol=1e-20)

# test uniqueness
tib = unitib

if (F){
  # scale variables 0 1
  #tib = tib %>% mutate_if(is.numeric, range01)
  
}
#cos = tcosine(tib)

#mat = gen_random_matrix('normal', 1000, 100)
#bigdist(as.matrix(mat), 'analysis/dist_matrix.tmp', method='euclidean')
#dist_uniq()

#utib$sum_dists_div = sum_dists / nrow(utib)
#utib$sum_dists_log = log10(utib$sum_dists+1)
#utib$sum_dists_sqrt = sqrt(utib$sum_dists)
#n = 3
#utib$sum_dists_nrt = utib$sum_dists ^ (1 / n)
#utib$uniq_lin = 1/(1+utib$sum_dists)
#utib$uniq_log = 1/(1+utib$sum_dists_log)
#utib$uniq_sqrt = 1/(1+utib$sum_dists_sqrt)
#utib$sum_cos = rowSums(cos)
#utib$sum_cos_pos = utib$sum_cos-min(utib$sum_cos)
#utib$sum_cos_log = log10(utib$sum_cos+1)

#View(round(utib,4))
#summary(utib)

# plot histograms
#tib_long <- utib %>% pivot_longer(colnames(utib)) %>% as_tibble()
#gp1 <- ggplot(tib_long, aes(x = value)) +    # Draw each column as histogram
  #geom_histogram(bins = 10) + 
#  geom_density(adjust = 1/2) + 
#  facet_wrap(~ name, scales = "free_x")
#print(gp1)

# characteristics of uniq to be saved for diagnostic
#x = scale(utib$sum_dists)

#desc_stats(x)
```


# Outlier detection

## Univariate OD

This is similar to outlier detection, in the sense of detection of extreme values, and not of measurement errors. 
For a single variable, there are well-known methods, e.g., boxplots, n StDev, etc.

"Enderlein (1987) goes even further as the author considers outliers as values that deviate so much from other observations one might suppose a different underlying sampling mechanism.
... it sometimes makes sense to formally distinguish two classes of outliers: (i) extreme values and (ii) mistakes."
https://statsandr.com/blog/outliers-detection-in-r

## Multivariate OD

In a multi-variate context, the outliers we are interested in are *rare combinations of values*. Individually, the values of each variable might not be extreme, but the combination appears relatively far from others.

"The adjusted quantile plot function of the mvoutlier package will solve for and order the squared Mahalanobis Distances for the given observations and plot them against the empirical Chi-Squared distribution function of these values."
https://rpubs.com/Treegonaut/301942


```{r eval=FALSE}
# use mvoutlier
indf = as.data.frame(gen_random_matrix('2groups', 20, 3))

outliers <- aq.plot(indf, delta=qchisq(0.975, df = ncol(indf)))
```


## Mahalanobis distance

The Mahalnobis distance is often used for OD in multivariate data.

"In multivariate space, the Mahalanobis distance is the distance between two points. 
It's frequently used to locate outliers in statistical investigations involving 
several variables."
Source: https://www.r-bloggers.com/2021/08/how-to-calculate-mahalanobis-distance-in-r/

```{r}
intib = gen_random_matrix('2groups', 10, 4)
intib

# x: input data
# center: indicate the mean vector of the distribution
# cov: indicate the covariance matrix of the distribution
intib$maha_dist = mahalanobis(intib, colMeans(intib), cov(intib))

# The p-value is the Chi-Square statistic of the
# Mahalanobis distance with k-1 degrees of freedom, 
# where k is the number of variables.
intib$maha_pvalue <- pchisq(intib$maha_dist, df=ncol(intib), lower.tail=F)
#View(intib)
intib$maha_dist
```

# Uniqueness

## Univariate uniqueness

Uniqueness can be thought of as $1 - p$, where p is the probability of encountering a particular observation from random extractions from a set.

For example, these are the percentages of land cover categories in the UK: Farmland 56.7%, Natural 34.9%, Green urban 2.5%, Built on 5.9%. Taking a probabilistic view, we can define the uniqueness of a category as 1-p. The rarest category we would encounter by selecting a random area in the UK is green urban ($U = .98$).

Data source: https://www.eea.europa.eu/publications/COR0-landcover

```{r}
uk_landcover = tibble(cat=c('farm','natural','green urban','built'), pc=c(56.7, 34.9, 2.5, 5.9))
uk_landcover$p = uk_landcover$pc/100
uk_landcover$uniq = 1 - uk_landcover$p

uk_landcover
```

To make it more interpretable, we can use the z score. This way, more unique observations emerge from the data.

```{r}
uk_landcover$uniq_z = round(zscore(uk_landcover$uniq),2)

uk_landcover
```

We can think of uniqueness as a deviation from the uniform distribution.
If all types of observation occur at the same probability, it is not possible to calculate a meaningful uniqueness score ($z$ is NA).

```{r}
uniform_tib = tibble(cat=c('cat1','cat2','cat3','cat4'), pc=1/4, uniq=1-1/4)
uniform_tib$uniq_z = zscore(uniform_tib$uniq)
print(uniform_tib)
```

We can use measures of entropy to see if the data has heterogeneity that can be used to quantify uniqueness. High entropy means low heterogeneity (no observation will be unique) and vice-versa.

```{r}
print(paste('Entropy of uniform data', round(entropy(uniform_tib$pc),2)))
print(paste('Entropy of heterogeneous data', round(entropy(uk_landcover$pc),2)))
```

## Multi-variate uniqueness

The multivariate case is more interesting and challenging. It is useful for exploratory data analysis (EDA).

In this example, we consider four observations along two dimensions. The geometric interpretation of uniqueness of observation $a$ in this case is $u_a = \sum dist(a,b)$. To make the results more interpretable, we can obtain the $z$ scores. It is possible to observe visually that point 1 is the most spatially isolated, while point 2 is the most central to the dataset and therefore has the lowest uniqueness.

### Minimal example

```{r}
multiv_tib = as_tibble(matrix(c(-1,0,1,0,-1,0,1,1), ncol = 2))
print(multiv_tib)

# plot
ggplot(multiv_tib, aes(x=V1, y=V2, label=rownames(multiv_tib))) + theme(aspect.ratio = 1) +
  geom_point(size=3, colour='blue', fill='blue') + geom_text(nudge_y = .1) + 
  ggtitle('Location of points')

# get distance matrix
dist_mat = as.matrix(dist(multiv_tib, diag = T, upper = T))
print(dist_mat)
# sum distances
multiv_tib$sum_dist = round(rowSums(dist_mat),2)
multiv_tib$sum_dist_z = round(zscore(multiv_tib$sum_dist),2)

# plot uniqueness
ggplot(multiv_tib, aes(x=V1, y=V2, label=sum_dist)) + theme(aspect.ratio = 1) +
  geom_point(size=3, colour='green', fill='green') + geom_text(nudge_y = .1) + 
  ggtitle('Sum of distances to other points')

# plot z scores of uniqueness 
ggplot(multiv_tib, aes(x=V1, y=V2, label=sum_dist_z)) + theme(aspect.ratio = 1) +
  geom_point(size=3, colour='green', fill='green') + geom_text(nudge_y = .1) + 
  ggtitle('Sum of distances to other points (z scores)')

```


### Distance-based uniq

Define uniqueness functions

```{r uniq}

#' Descriptive statistics
#' @return tibble with a row with descriptive stats about x (z scores).
desc_stats_uniq <- function(utib){
  x = utib$sum_dists_z
  shap_x = x
  if (nrow(utib) > 5000){
    shap_x = sample(utib, 5000)
  }
  
  uclasses = table(utib$sum_dists_class)/nrow(utib)*100
  
  tibble(
    n = length(x),
    min = min(x),
    med = median(x),
    mean = mean(x),
    max = max(x),
    pc_very_common = uclasses['very common'],
    pc_common = uclasses['common'],
    pc_inbet = uclasses['in between'],
    pc_rare = uclasses['rare'],
    pc_very_rare = uclasses['very rare'],
    skewness = round(skewness(x),3),
    iqr = round(IQR(x),3),
    shapiro_w = shapiro.test(shap_x)$stat,
    shapiro_p = round(shapiro.test(shap_x)$p,5)
  )
}

#' Find a sample size to produce a distance matrix that is manageable in memory
find_sample_sz = function(n_row, n_col, max_dist_size){
  while(((n_col*n_row)^2) > max_dist_size){
    n_row = n_row - 1
  }
  return(n_row)
}

#' Classify p values returned by the uniqueness function with a 
#' human readable label.
classify_uniq = function(p_values){
  breaks = c(0, .001, .01, .05, .1, Inf)
  labels = c('very rare','rare','in between','common','very common')
  classif = cut(p_values, breaks, labels)
  #print(summary(classif)/length(p_values)*100)
  return(classif)
}

#' Calculate uniqueness based on distance in multi-dimensional space.
#' @param tib
#' @param dist_method
#' @return 
dist_uniq = function(tib, dist_method){
  print(paste('dist_uniq', nrow(tib), ncol(tib), dist_method))
  # scale input variables mean + sd
  tib = tib %>% mutate_if(is.numeric, scale)
  stopifnot(dist_method %in% c('euclidean','mahalanobis','manhattan', 'minkowski'))
  
  
  # check memory limit for distance matrix
  max_matrix_sz = 1e8
  #print(dim(tib))
  sample_rows = find_sample_sz(nrow(tib), ncol(tib), max_matrix_sz)
  if (nrow(tib) > sample_rows){
    warning(paste("Distance matrix would be too large, using a sample N =",sample_rows))
    tib = tib %>% sample_n(sample_rows)
  }
  # calculate distances
  if (dist_method == 'mahalanobis'){
    # tolerance has to be set to avoid rounding errors
    sum_dists = mahalanobis(tib, colMeans(tib), cov(tib), tol=1e-20)
  } else {
    sum_dists = rowSums(as_tibble(as.matrix(dist(tib, 
                    diag = T, upper = T, method = dist_method))))
  }
  utib = tib
  utib$sum_dists = sum_dists
  utib$sum_dists_z = zscore(sum_dists)
  # calculate p value of z score: 
  # we are interested to check if an observation is higher than expected (lower.tail=F) 
  utib$sum_dists_z_p = pnorm(q=utib$sum_dists_z, mean = mean(utib$sum_dists_z), 
                 sd = sd(utib$sum_dists_z), lower.tail=F)
  utib$sum_dists_class = classify_uniq(utib$sum_dists_z_p)
  # discarded p values
  #utib$sum_dists_z_p = 2*pnorm(q=utib$sum_dists_z, lower.tail=FALSE)
  #utib$sum_dists_pvalue <- pchisq(utib$sum_dists_z, df=ncol(tib), lower.tail=F)
  utib$dist_method = dist_method
  as_tibble(utib)
}

#' Main function to calculate uniqueness 
#' @tib input data tibble, including numeric and non-numeric columns
#' @return a tibble with results and the similarity matrix between all rows.
OLD_calc_uniqueness = function(tib, sim_method, handle_missing_values=T){
  # get only numeric columns
  indata = tib %>% select_if(is.numeric)
  stopifnot(ncol(indata) > 1)
  
  stopifnot(sim_method %in% c('cosine', "euclidean", "maximum", "manhattan",
                              "canberra", "binary", "minkowski"))
  
  # calculate entropy for each row: 
  #   high = uniform data, low = some attribute high/low
  #entropies = apply(X = indata, MARGIN = 1, FUN = function(x) entropy(x[!is.na(x)]))
  
  if (sim_method == 'cosine'){
    # scale columns between 0 and 1
    scaled_indata = indata %>% mutate_if(is.numeric, range01)
    # calculate cosine similarities between rows
    sim_matrix = tcosine(scaled_indata) %>% as_tibble()
  } else {
    stopifnot(sim_method %in% c("euclidean", "maximum", "manhattan", "canberra", 
              "binary", "minkowski"))
    # calculate cosine similarities between rows
    edist = dist(indata, diag=T, upper=T, method=sim_method) %>% as.matrix() %>% as_tibble()
    stopifnot(edist >= 0)
    sim_matrix = 1/(1+edist)
  }
  #View(sim_matrix)
  stopifnot(sim_matrix >= 0 & sim_matrix <= 1)
  
  if (handle_missing_values){
    # calculate cosine for cases with missing values
    missing_cases = rowSums(is.na(sim_matrix))
    missing_cases_row_idx = which(missing_cases == (ncol(sim_matrix)-1))
    #print(paste0('Missing cases N = ', length(missing_cases_row_idx)))
    for (row_idx in missing_cases_row_idx){
      for (other_row_idx in seq(nrow(indata))){
        non_null_pair = indata[c(row_idx,other_row_idx),] %>% 
          select_if(is.numeric) %>% select_if(~ !any(is.na(.)))
        n_pair_vars = ncol(non_null_pair)
        pair_cosine = tcosine(non_null_pair)
        sim_matrix[row_idx, other_row_idx] = pair_cosine[1,2]
        sim_matrix[other_row_idx, row_idx] = pair_cosine[2,1]
      }
    }
  }
  uniq_df = tib
  uniq_df$na_count = rowSums(is.na(indata))
  # calculate uniqueness
  uniq_df$sim_sum = rowSums(sim_matrix, na.rm=T)-1
  uniq_df$sim_sum[uniq_df$sim_sum == 0] <- NA
  uniq_df$uniq = 1 - (uniq_df$sim_sum / nrow(uniq_df))
  uniq_df$uniq_rank = rank(-uniq_df$uniq)
  uniq_df$uniq_z = z_score(uniq_df$uniq)
  uniq_df$uniq_dist_avg = uniq_df$uniq - mean(uniq_df$uniq)
  # save entropy in results
  #uniq_df$entropy = entropies
  #View(uniq_df)
  #print(summary(uniq_df$uniq))
  stopifnot(uniq_df$uniq >= 0 & uniq_df$uniq <= 1)
  return(list(uniqueness=uniq_df, sim_matrix=sim_matrix))
}
```

### Experiment with high dim

On high dimensional data, the interpretation of $u$ is less immediate.
Therefore, it is useful to observe the behaviour of the uniqueness measure $u$ 
through a Monte Carlo method.

A core idea is that data with uniformly and normally distributed variables can still produce high values of $u$, generating false positives in the search for unique observations.
The Shapiro-Wilk test on $u$ consistently returns low W on heterogeneous distributions with actual groups and high W on homogeneous data, regardless of scale.

#### Generate synth data

```{r eval=FALSE}
#t = gen_random_matrix('normal', 10, 5)
#u = calc_uniqueness(t, 'euclidean')
#View(u$uniqueness)
#ggplot(u$uniqueness, aes(x=uniq_dist_avg)) + xlab("uniq_dist_avg") + 
#  geom_histogram(color="white", fill="lightgreen", bins = 10)#
#ggplot(u$uniqueness, aes(x=uniq)) + #xlab("uniq_dist_avg") + 
#  geom_histogram(color="white", fill="gray", bins = 10)

if (T){
  # synthetic data
  res = tibble()
  set.seed(NULL)
  for (rand_data in c('normal','uniform','2groups','3groups'))
  for (dist_method in c('euclidean','manhattan','minkowski','mahalanobis'))
  for (data_scale in c(1,100,1000))
  for (row_n in c(10,100,1000,10000))
  for (col_n in c(2,10,20))
  for (trial in seq(10))
  {
    if (row_n == 0 | col_n == 0) next()
    #if (dist_method != 'mahalanobis') next()
    rand_tib = gen_random_matrix(rand_data, row_n, col_n, data_scale)
    utib = dist_uniq(rand_tib, dist_method)
    res_row = desc_stats_uniq(utib)
    res_row = res_row %>% add_column(distrib = rand_data, 
                             dist_method = dist_method,
                             data_scale = data_scale,
                             row_n = row_n, 
                             col_n = col_n, 
                             sample_n = nrow(utib),
                             trial = trial,
                             .before = "n")
    res = bind_rows(res, res_row)
  }
  
  saveRDS(res, 'analysis/high_dimensional_experiment_1.rds')
  openxlsx::write.xlsx(res, 'analysis/high_dimensional_experiment_1.xlsx')
}
```

#### Analyse results

Summarise experiment 1 results to observe trends in $u$ in different settings.

```{r}
res_tib = readRDS('analysis/high_dimensional_experiment_1.rds') 
  #%>% #mutate_if(is.character, factor)
print(nrow(res_tib))

summary_res = tibble()

for (cols in list(c('row_n'), c('col_n'), 
    c('distrib','row_n'), c('distrib','col_n'), c('distrib'),
    c('dist_method'), c('data_scale'),
    c('distrib','dist_method'),
    c('distrib','dist_method','data_scale'))){
  print(cols)
  cols_str = paste(cols, collapse = ' ')
  res_stats_tib = res_tib %>% 
    #group_by(distrib, dist_method) %>% 
    group_by(across(all_of(cols))) %>%  # all_of
    dplyr::summarise(cols = cols_str, 
      n = n(), 
      mn_dist_min = mean(min),
      mn_dist_max = mean(max),
      mn_vcommon = mean(pc_very_common),
      mn_common = mean(pc_common),
      mn_inbet = mean(pc_inbet),
      mn_rare = mean(pc_rare),
      mn_vrare = mean(pc_very_rare),
      mn_skew = mean(skewness)) %>% 
    mutate_if(is.numeric, round, 2)
  
  summary_res = bind_rows(summary_res, res_stats_tib)
}
summary_res
```

Impact of variables on distribution of uniqueness values.

Different distributions generate different common/rare distribution. 1,440 cases observed for each combination. Uniform and normally-distributed data generate a similar pattern, with normal data with more having more rare/very rare observations (2%, 1.1%). Uneven distributions with groups, as expected by their design, generate more rare/very rare observations (4.1% and 7.3%), having a large central group of close observations and relatively many distant observations:

Using the Dunn's test, the only two distributions that are not significantly different are the normal and the uniform, which generate similar uniqueness patterns (TODO: why?).

```{r}
# impact of data distribution
#summary_res %>% filter(cols == 'distrib') %>% dplyr::select(distrib, n, mn_dist_min, mn_dist_max, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% flextable()
summary_results_tib = summary_res

# compare groups by number of rows
tib = summary_results_tib %>% filter(cols == 'distrib') %>% 
  dplyr::select(distrib, n, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% unique()
tib %>% flextable()

all_vals = tibble()
u_labels = c('vcommon','common','inbet','rare','vrare')
for(i in tib$distrib){
  pcs = tib %>% filter(distrib == i) %>% 
    dplyr::select(mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% t() %>% as.vector()
  sz = tib %>% filter(distrib == i) %>% dplyr::select(n) %>% .[[1]]
  vals = generate_cat_data_from_pc(pcs, u_labels, sz)
  row_tib = tibble(group = i, val = vals)
  all_vals = bind_rows(all_vals, row_tib)
}

# Dunn's test to check if the groups are different (non-parametric)
res_dunn = all_vals %>% rstatix::dunn_test(val ~ group)
res_dunn
```

The size of the dataset (n observations x n variables) has minimal impact on the behaviour of $u$.
Varying the number of observations from 10 to 10,000, it is possible to observe that with very few observations (10) it is less likely to obtain very rare observations, as expected. The other three rows converge and do not have significant differences (using Dunn's test):

Rows:
```{r}
# compare groups by number of rows
tib = summary_results_tib %>% filter(cols == 'row_n') %>% 
  dplyr::select(row_n, n, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare)
all_vals = tibble()
u_labels = c('vcommon','common','inbet','rare','vrare')
for(i in tib$row_n){
  pcs = tib %>% filter(row_n == i) %>% 
    dplyr::select(mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% t() %>% as.vector()
  sz = tib %>% filter(row_n == i) %>% dplyr::select(n) %>% .[[1]]
  vals = generate_cat_data_from_pc(pcs, u_labels, sz)
  row_tib = tibble(group = i, val = vals)
  all_vals = bind_rows(all_vals, row_tib)
}

# Dunn's test to check if the groups are different (non-parametric)
res_dunn = all_vals %>% rstatix::dunn_test(val ~ group)
res_dunn
```
Columns do not have impact on the results:

```{r}
# compare groups by number of rows
tib = summary_results_tib %>% filter(cols == 'col_n') %>% 
  dplyr::select(col_n, n, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare)
all_vals = tibble()
u_labels = c('vcommon','common','inbet','rare','vrare')
for(i in tib$col_n){
  print(i)
  pcs = tib %>% filter(col_n == i) %>% 
    dplyr::select(mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% t() %>% as.vector()
  sz = tib %>% filter(col_n == i) %>% dplyr::select(n) %>% .[[1]]
  print(paste('generating synth data:',sz))
  vals = generate_cat_data_from_pc(pcs, u_labels, sz)
  row_tib = tibble(group = i, val = vals)
  all_vals = bind_rows(all_vals, row_tib)
}

# Dunn's test to check if the groups are different (non-parametric)
res_dunn = all_vals %>% rstatix::dunn_test(val ~ group)
res_dunn
```
The four distance methods do not generate significantly different proportions of $u$ results.

```{r}
# compare distance metrics
tib = summary_results_tib %>% filter(cols == 'dist_method') %>% 
  dplyr::select(dist_method, n, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% unique()
all_vals = tibble()
u_labels = c('vcommon','common','inbet','rare','vrare')
for(i in tib$dist_method){
  pcs = tib %>% filter(dist_method == i) %>% 
    dplyr::select(mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% t() %>% as.vector()
  sz = tib %>% filter(dist_method == i) %>% dplyr::select(n) %>% .[[1]]
  vals = generate_cat_data_from_pc(pcs, u_labels, sz)
  row_tib = tibble(group = i, val = vals)
  all_vals = bind_rows(all_vals, row_tib)
}

# Dunn's test to check if the groups are different (non-parametric)
res_dunn = all_vals %>% rstatix::dunn_test(val ~ group)
res_dunn
```
Data scale don't make any difference to the results either.

```{r}
# compare data scale
tib = summary_results_tib %>% filter(cols == 'data_scale') %>% 
  dplyr::select(data_scale, n, mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% unique()
all_vals = tibble()
u_labels = c('vcommon','common','inbet','rare','vrare')
for(i in tib$data_scale){
  pcs = tib %>% filter(data_scale == i) %>% 
    dplyr::select(mn_vcommon, mn_common, mn_inbet, mn_rare, mn_vrare) %>% t() %>% as.vector()
  sz = tib %>% filter(data_scale == i) %>% dplyr::select(n) %>% .[[1]]
  vals = generate_cat_data_from_pc(pcs, u_labels, sz)
  row_tib = tibble(group = i, val = vals)
  all_vals = bind_rows(all_vals, row_tib)
}

# Dunn's test to check if the groups are different (non-parametric)
res_dunn = all_vals %>% rstatix::dunn_test(val ~ group)
res_dunn
```

# Case studies

## London (land use and POIs)

```{r}

```

## London (boroughs)

```{r}
lnd_boro = st_read('data/london/london_borough_profiles_2015/london_boroughs_profiles_2015.geojson')
# select variables
print(names(lnd_boro))
lnd_boro = lnd_boro %>% select(gss, name, population_density_per_hectare_2017, average_age_2017, pc_of_resident_population_born_abroad_2015, employment_rate_pc_2015, modelled_household_median_income_estimates_2012_13, proportion_of_seats_won_by_conservatives_in_2014_election, proportion_of_seats_won_by_labour_in_2014_election, median_house_price_2015) %>% rename(id = gss)
plot(lnd_boro %>% select(-id,-name), border='white')

# integrate missing data
# from https://data.gov.uk/dataset/248f5f04-23cf-4470-9216-0d0be9b877a8/london-borough-profiles-and-atlas/datafile/1e5d5226-a1a7-4097-afbf-d3bd39226c39/preview
#lnd_boro[lnd_boro$name == "Kensington and Chelsea", "gross_annual_pay_2016"] = 63620
```
### Prep data

Find problematic variables.

```{r}
# get correlations
lnd_boro_corr = lnd_boro %>% st_drop_geometry() %>% select(-name,-id) %>% correlate(method='kendall') %>% stretch()
# observe high correlations
lnd_boro_corr %>% filter(r > .5 | r < -.5)
# remove Labour variable
lnd_boro = lnd_boro %>% select(-proportion_of_seats_won_by_labour_in_2014_election)
```

```{r}
lnd_boro_desc_stats = t(stat.desc(lnd_boro %>% st_drop_geometry() %>% select(-id,-name), norm=T))
lnd_boro_desc_stats
```

```{r}
# histograms to see normality
ggplot(gather(lnd_boro %>% st_drop_geometry() %>% select(-name,-id)), aes(value)) + 
    geom_histogram(bins = 20) + 
    facet_wrap(~key, scales = 'free_x')
```
The only variable whose skewness is a concern is `median_house_price_2015`. It will be normalised with log10.

```{r}
# normalise and scale
lnd_boro$median_house_price_2015_log10 = log10(lnd_boro$median_house_price_2015+1)
lnd_boro = lnd_boro %>% select(-median_house_price_2015)

# skewness is now acceptable
skewness(lnd_boro$median_house_price_2015_log10)
```

Scale all numeric variables.

```{r}
lnd_boro_scaled = lnd_boro %>% mutate_if(is.numeric, scale) %>% mutate_if(is.numeric, range01)
lnd_boro_scaled %>% head(5)
```

Multidimensional scaling of boroughs.

```{r}
mds <- lnd_boro %>% select(-name,-id) %>% st_drop_geometry() %>%
  dist() %>% cmdscale() %>% as_tibble()
colnames(mds) <- c("Dim1", "Dim2")
mds$name = lnd_boro_scaled$name

# plot MDS
ggplot(mds, aes(x=Dim1, y=Dim2)) +
  geom_point() +
  geom_text(
    label=mds$name, 
    nudge_x = .25, nudge_y = .25, 
    check_overlap = T
  )
rm(mds)
```


### Calc uniqueness

Calculate uniqueness of boroughs based on all variables.

```{r}
#indata = lnd_boro_scaled %>% st_drop_geometry() %>% select(-name,-id)


#openxlsx::write.xlsx(lnd_boro_uniq %>% st_drop_geometry() %>% mutate_if(is.numeric, round, 4), 'analysis/london_boroughs_scaled_uniqueness.xlsx')

```

### Viz uniqueness 

```{r}
#lnd_boro_uniq %>% select(uniq) %>% plot(border = 'white', nbreaks = 10, breaks='pretty')
```



## Countries (World Bank data)

```{r}

```

## EU (Eurostat data)


```{r}

```

End of notebook

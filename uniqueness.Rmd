---
title: "Calculating uniqueness"
author: "Andrea Ballatore"
date: "27/03/2022"
output: html_document
---
# Setup 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(moments)
library(knitr)
library(corrr)
library(entropy)
library(pastecs)
library(coop)
library(sf)
```

# Uniqueness functions

```{r uniq}

#' Main function to calculate uniqueness 
#' @tib input data tibble, including numeric and non-numeric columns
#' @return a tibble with results and the similarity matrix between all rows.
calc_uniqueness = function(tib, sim_method, handle_missing_values=T){
  # get only numeric columns
  indata = tib %>% select_if(is.numeric)
  stopifnot(ncol(indata) > 1)
  
  stopifnot(sim_method %in% c('cosine', "euclidean", "maximum", "manhattan",
                              "canberra", "binary", "minkowski"))
  
  # calculate entropy for each row: 
  #   high = uniform data, low = some attribute high/low
  #entropies = apply(X = indata, MARGIN = 1, FUN = function(x) entropy(x[!is.na(x)]))
  
  if (sim_method == 'cosine'){
    # scale columns between 0 and 1
    scaled_indata = indata %>% mutate_if(is.numeric, range01)
    # calculate cosine similarities between rows
    sim_matrix = tcosine(scaled_indata) %>% as_tibble()
  } else {
    stopifnot(sim_method %in% c("euclidean", "maximum", "manhattan", "canberra", 
              "binary", "minkowski"))
    # calculate cosine similarities between rows
    edist = dist(indata, diag=T, upper=T, method=sim_method) %>% as.matrix() %>% as_tibble()
    stopifnot(edist >= 0)
    sim_matrix = 1/(1+edist)
  }
  #View(sim_matrix)
  stopifnot(sim_matrix >= 0 & sim_matrix <= 1)
  
  if (handle_missing_values){
    # calculate cosine for cases with missing values
    missing_cases = rowSums(is.na(sim_matrix))
    missing_cases_row_idx = which(missing_cases == (ncol(sim_matrix)-1))
    #print(paste0('Missing cases N = ', length(missing_cases_row_idx)))
    for (row_idx in missing_cases_row_idx){
      for (other_row_idx in seq(nrow(indata))){
        non_null_pair = indata[c(row_idx,other_row_idx),] %>% 
          select_if(is.numeric) %>% select_if(~ !any(is.na(.)))
        n_pair_vars = ncol(non_null_pair)
        pair_cosine = tcosine(non_null_pair)
        sim_matrix[row_idx, other_row_idx] = pair_cosine[1,2]
        sim_matrix[other_row_idx, row_idx] = pair_cosine[2,1]
      }
    }
  }
  uniq_df = tib
  uniq_df$na_count = rowSums(is.na(indata))
  # calculate uniqueness
  uniq_df$sim_sum = rowSums(sim_matrix, na.rm=T)-1
  uniq_df$sim_sum[uniq_df$sim_sum == 0] <- NA
  uniq_df$uniq = 1 - (uniq_df$sim_sum / nrow(uniq_df))
  uniq_df$uniq_rank = rank(-uniq_df$uniq)
  uniq_df$uniq_z = (uniq_df$uniq - mean(uniq_df$uniq))/sd(uniq_df$uniq)
  uniq_df$uniq_dist_avg = uniq_df$uniq - mean(uniq_df$uniq)
  # save entropy in results
  #uniq_df$entropy = entropies
  #View(uniq_df)
  #print(summary(uniq_df$uniq))
  stopifnot(uniq_df$uniq >= 0 & uniq_df$uniq <= 1)
  return(list(uniqueness=uniq_df, sim_matrix=sim_matrix))
}

range01 <- function(x){
  (x-min(x,na.rm = T))/(max(x,na.rm = T)-min(x,na.rm = T))
}

#' @return a tibble with random values following a given distribution
gen_random_matrix = function(distrib, n_row, n_col, scale=1, group_split=.2){
  print(paste('gen_random_matrix',distrib, n_row, n_col, scale))
  stopifnot(distrib %in% c('normal','uniform','groups'))
  stopifnot(n_row > 0 && n_col > 0)
  stopifnot(group_split > 0 && group_split < 1)
  if (distrib=='normal'){
    rand = as_tibble(matrix(rnorm(n_row*n_col), nrow=n_row))
    return(rand*scale)
  }
  if (distrib=='uniform'){
    return(as_tibble(matrix(runif(n_row*n_col), nrow=n_row))*scale)
  }
  if (distrib=='groups'){
    # create data with some common and rare groups
    common_tib = gen_random_matrix('normal', n_row * (1-group_split), n_col, scale)
    rare_tib =   gen_random_matrix('normal', n_row * group_split, n_col, scale*1000)
    tib = bind_rows(common_tib, rare_tib)
    return(tib %>% sample_frac(1))
  }
}
```

# Synthetic data

```{r}

#tibs = list(gen_random_matrix('normal', 10, 5),
#  gen_random_matrix('uniform', 10, 5),
#  gen_random_matrix('groups', 10, 5, 1))
#for (t in tibs){
#  u = calc_uniqueness(t, 'euclidean')
#  View(u$uniq)
#}

#t = gen_random_matrix('normal', 10, 5)
#u = calc_uniqueness(t, 'euclidean')
#View(u$uniqueness)
#ggplot(u$uniqueness, aes(x=uniq_dist_avg)) + xlab("uniq_dist_avg") + 
#  geom_histogram(color="white", fill="lightgreen", bins = 10)#

#ggplot(u$uniqueness, aes(x=uniq)) + #xlab("uniq_dist_avg") + 
#  geom_histogram(color="white", fill="gray", bins = 10)

# synthetic data
res = tibble()
for (method in c('cosine','euclidean','manhattan')) # 'minkowski',
for (rand_data in c('normal','uniform','groups'))
for (data_scale in c(1,100,10000))
for (trial in seq(10))
for (row_n in seq(0,10,10)){
  for (col_n in seq(0,5,5)){
    if (row_n == 0 | col_n == 0) next()
    rand_tib = gen_random_matrix(rand_data, row_n, col_n, data_scale)
    uniq = calc_uniqueness(rand_tib, method)
    #print(paste('corr(uniq,entropy) =', 
    #      round(cor(uniq$uniqueness$uniq, uniq$uniqueness$entropy),3)))
    uniq_dist_avg = round(uniq$uniqueness$uniq_dist_avg, 4)
    res = bind_rows(res, tibble(trial=trial, 
            rand_data=rand_data, data_scale=data_scale, method=method, 
            row_n = row_n, col_n = col_n,
            umin = min(uniq_dist_avg), umed = median(uniq_dist_avg), 
            umax = max(uniq_dist_avg), uiqr = IQR(uniq_dist_avg),
            uskew = round(skewness(uniq_dist_avg),2), ukurt = round(kurtosis(uniq_dist_avg),2)))
  }
}
View(res)

TODO
ggplot(uniq$uniqueness, aes(x=uniq)) + xlim(0,1) + xlab("Uniqueness index") + 
  geom_histogram(color="white", fill="lightblue", bins = 20)

ggplot(uniq$uniqueness, aes(x=entropy)) + xlab("Entropy") + 
  geom_histogram(color="white", fill="lightgreen", bins = 10)



```

# Case studies

## Greater London (land use and Points of Interest)

```{r}

```

## Greater London (boroughs)

```{r}
lnd_boro = st_read('data/london/london_borough_profiles_2015/london_boroughs_profiles_2015.geojson')
# select variables
print(names(lnd_boro))
lnd_boro = lnd_boro %>% select(gss, name, population_density_per_hectare_2017, average_age_2017, pc_of_resident_population_born_abroad_2015, employment_rate_pc_2015, modelled_household_median_income_estimates_2012_13, proportion_of_seats_won_by_conservatives_in_2014_election, proportion_of_seats_won_by_labour_in_2014_election, median_house_price_2015) %>% rename(id = gss)
plot(lnd_boro %>% select(-id,-name), border='white')

# integrate missing data
# from https://data.gov.uk/dataset/248f5f04-23cf-4470-9216-0d0be9b877a8/london-borough-profiles-and-atlas/datafile/1e5d5226-a1a7-4097-afbf-d3bd39226c39/preview
#lnd_boro[lnd_boro$name == "Kensington and Chelsea", "gross_annual_pay_2016"] = 63620
```
### Prep data

Find problematic variables.

```{r}
# get correlations
lnd_boro_corr = lnd_boro %>% st_drop_geometry() %>% select(-name,-id) %>% correlate(method='kendall') %>% stretch()
# observe high correlations
lnd_boro_corr %>% filter(r > .5 | r < -.5)
# remove Labour variable
lnd_boro = lnd_boro %>% select(-proportion_of_seats_won_by_labour_in_2014_election)
```

```{r}
lnd_boro_desc_stats = t(stat.desc(lnd_boro %>% st_drop_geometry() %>% select(-id,-name), norm=T))
lnd_boro_desc_stats
```

```{r}
# histograms to see normality
ggplot(gather(lnd_boro %>% st_drop_geometry() %>% select(-name,-id)), aes(value)) + 
    geom_histogram(bins = 20) + 
    facet_wrap(~key, scales = 'free_x')
```
The only variable whose skewness is a concern is `median_house_price_2015`. It will be normalised with log10.

```{r}
# normalise and scale
lnd_boro$median_house_price_2015_log10 = log10(lnd_boro$median_house_price_2015+1)
lnd_boro = lnd_boro %>% select(-median_house_price_2015)

# skewness is now acceptable
skewness(lnd_boro$median_house_price_2015_log10)
```

Scale all numeric variables.

```{r}
lnd_boro_scaled = lnd_boro %>% mutate_if(is.numeric, scale) %>% mutate_if(is.numeric, range01)
lnd_boro_scaled %>% head(5)
```

Multidimensional scaling of boroughs.

```{r}
mds <- lnd_boro %>% select(-name,-id) %>% st_drop_geometry() %>%
  dist() %>% cmdscale() %>% as_tibble()
colnames(mds) <- c("Dim1", "Dim2")
mds$name = lnd_boro_scaled$name

# plot MDS
ggplot(mds, aes(x=Dim1, y=Dim2)) +
  geom_point() +
  geom_text(
    label=mds$name, 
    nudge_x = .25, nudge_y = .25, 
    check_overlap = T
  )
rm(mds)
```


### Calc uniqueness

Calculate uniqueness of boroughs based on all variables.

```{r}
#indata = lnd_boro_scaled %>% st_drop_geometry() %>% select(-name,-id)


#openxlsx::write.xlsx(lnd_boro_uniq %>% st_drop_geometry() %>% mutate_if(is.numeric, round, 4), 'analysis/london_boroughs_scaled_uniqueness.xlsx')

```

### Viz uniqueness 

```{r}
lnd_boro_uniq %>% select(uniq) %>% plot(border = 'white', nbreaks = 10, breaks='pretty')
```



## Countries (World Bank data)

```{r}

```

## EU (Eurostat data)


```{r}

```

End of notebook

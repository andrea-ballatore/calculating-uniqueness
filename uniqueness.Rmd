---
title: "Calculating uniqueness"
author: "Andrea Ballatore"
date: "27/03/2022"
output: html_document
---
# Setup 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(moments)
library(knitr)
library(corrr)
library(entropy)
library(pastecs)
library(coop)
library(sf)
```

# Uniqueness functions

```{r uniq}

#' Main function to calculate uniqueness 
#' @tib input data tibble, including numeric and non-numeric columns
#' @return a tibble with results and the similarity matrix between all rows.
calc_uniqueness = function(tib, sim_method, handle_missing_values=T){
  # get only numeric columns
  indata = tib %>% select_if(is.numeric)
  stopifnot(ncol(indata) > 1)
  
  stopifnot(sim_method %in% c('cosine', "euclidean", "maximum", "manhattan",
                              "canberra", "binary", "minkowski"))
  
  # calculate entropy for each row: 
  #   high = uniform data, low = some attribute high/low
  #entropies = apply(X = indata, MARGIN = 1, FUN = function(x) entropy(x[!is.na(x)]))
  
  if (sim_method == 'cosine'){
    # scale columns between 0 and 1
    scaled_indata = indata %>% mutate_if(is.numeric, range01)
    # calculate cosine similarities between rows
    sim_matrix = tcosine(scaled_indata) %>% as_tibble()
  } else {
    stopifnot(sim_method %in% c("euclidean", "maximum", "manhattan", "canberra", 
              "binary", "minkowski"))
    # calculate cosine similarities between rows
    edist = dist(indata, diag=T, upper=T, method=sim_method) %>% as.matrix() %>% as_tibble()
    stopifnot(edist >= 0)
    sim_matrix = 1/(1+edist)
  }
  #View(sim_matrix)
  stopifnot(sim_matrix >= 0 & sim_matrix <= 1)
  
  if (handle_missing_values){
    # calculate cosine for cases with missing values
    missing_cases = rowSums(is.na(sim_matrix))
    missing_cases_row_idx = which(missing_cases == (ncol(sim_matrix)-1))
    #print(paste0('Missing cases N = ', length(missing_cases_row_idx)))
    for (row_idx in missing_cases_row_idx){
      for (other_row_idx in seq(nrow(indata))){
        non_null_pair = indata[c(row_idx,other_row_idx),] %>% 
          select_if(is.numeric) %>% select_if(~ !any(is.na(.)))
        n_pair_vars = ncol(non_null_pair)
        pair_cosine = tcosine(non_null_pair)
        sim_matrix[row_idx, other_row_idx] = pair_cosine[1,2]
        sim_matrix[other_row_idx, row_idx] = pair_cosine[2,1]
      }
    }
  }
  uniq_df = tib
  uniq_df$na_count = rowSums(is.na(indata))
  # calculate uniqueness
  uniq_df$sim_sum = rowSums(sim_matrix, na.rm=T)-1
  uniq_df$sim_sum[uniq_df$sim_sum == 0] <- NA
  uniq_df$uniq = 1 - (uniq_df$sim_sum / nrow(uniq_df))
  # save entropy in results
  #uniq_df$entropy = entropies
  #View(uniq_df)
  #print(summary(uniq_df$uniq))
  stopifnot(uniq_df$uniq >= 0 & uniq_df$uniq <= 1)
  return(list(uniqueness=uniq_df, sim_matrix=sim_matrix))
}

range01 <- function(x){
  (x-min(x,na.rm = T))/(max(x,na.rm = T)-min(x,na.rm = T))
}
```

# Synthetic data

```{r}

# synthetic data
res = tibble()
for (method in c('cosine','euclidean','minkowski','manhattan'))
for (trial in seq(10))
for (row_n in seq(0,100,100)){
  for (col_n in seq(0,100,100)){
    if (row_n == 0 | col_n == 0) next()
    #print(paste('Random matrix:', method,', obs', row_n, 'dim', col_n))
    rand_tib = as_tibble(matrix(rnorm(row_n*col_n), nrow=row_n))
    uniq = calc_uniqueness(rand_tib, method)
    #print(paste('corr(uniq,entropy) =', 
    #      round(cor(uniq$uniqueness$uniq, uniq$uniqueness$entropy),3)))
    uniq = round(uniq$uniqueness$uniq,3)
    res = bind_rows(res, tibble(trial=trial, method=method, row_n = row_n, col_n = col_n,
            min(uniq), median(uniq), max(uniq), 
            round(skewness(uniq),2), round(kurtosis(uniq),2)))
  }
}
View(res)

TODO
ggplot(uniq$uniqueness, aes(x=uniq)) + xlim(0,1) + xlab("Uniqueness index") + 
  geom_histogram(color="white", fill="lightblue", bins = 20)

ggplot(uniq$uniqueness, aes(x=entropy)) + xlab("Entropy") + 
  geom_histogram(color="white", fill="lightgreen", bins = 10)



```

# Case studies

## Greater London (land use and Points of Interest)

```{r}

```

## Greater London (boroughs)

```{r}
lnd_boro = st_read('data/london/london_borough_profiles_2015/london_boroughs_profiles_2015.geojson')
# select variables
print(names(lnd_boro))
lnd_boro = lnd_boro %>% select(gss, name, population_density_per_hectare_2017, average_age_2017, pc_of_resident_population_born_abroad_2015, employment_rate_pc_2015, modelled_household_median_income_estimates_2012_13, proportion_of_seats_won_by_conservatives_in_2014_election, proportion_of_seats_won_by_labour_in_2014_election, median_house_price_2015) %>% rename(id = gss)
plot(lnd_boro %>% select(-id,-name), border='white')

# integrate missing data
# from https://data.gov.uk/dataset/248f5f04-23cf-4470-9216-0d0be9b877a8/london-borough-profiles-and-atlas/datafile/1e5d5226-a1a7-4097-afbf-d3bd39226c39/preview
#lnd_boro[lnd_boro$name == "Kensington and Chelsea", "gross_annual_pay_2016"] = 63620
```
### Prep data

Find problematic variables.

```{r}
# get correlations
lnd_boro_corr = lnd_boro %>% st_drop_geometry() %>% select(-name,-id) %>% correlate(method='kendall') %>% stretch()
# observe high correlations
lnd_boro_corr %>% filter(r > .5 | r < -.5)
# remove Labour variable
lnd_boro = lnd_boro %>% select(-proportion_of_seats_won_by_labour_in_2014_election)
```

```{r}
lnd_boro_desc_stats = t(stat.desc(lnd_boro %>% st_drop_geometry() %>% select(-id,-name), norm=T))
lnd_boro_desc_stats
```

```{r}
# histograms to see normality
ggplot(gather(lnd_boro %>% st_drop_geometry() %>% select(-name,-id)), aes(value)) + 
    geom_histogram(bins = 20) + 
    facet_wrap(~key, scales = 'free_x')
```
The only variable whose skewness is a concern is `median_house_price_2015`. It will be normalised with log10.

```{r}
# normalise and scale
lnd_boro$median_house_price_2015_log10 = log10(lnd_boro$median_house_price_2015+1)
lnd_boro = lnd_boro %>% select(-median_house_price_2015)

# skewness is now acceptable
skewness(lnd_boro$median_house_price_2015_log10)
```

Scale all numeric variables.

```{r}
lnd_boro_scaled = lnd_boro %>% mutate_if(is.numeric, scale) %>% mutate_if(is.numeric, range01)
lnd_boro_scaled %>% head(5)
```

Multidimensional scaling of boroughs.

```{r}
mds <- lnd_boro %>% select(-name,-id) %>% st_drop_geometry() %>%
  dist() %>% cmdscale() %>% as_tibble()
colnames(mds) <- c("Dim1", "Dim2")
mds$name = lnd_boro_scaled$name

# plot MDS
ggplot(mds, aes(x=Dim1, y=Dim2)) +
  geom_point() +
  geom_text(
    label=mds$name, 
    nudge_x = .25, nudge_y = .25, 
    check_overlap = T
  )
rm(mds)
```


### Calc uniqueness

Calculate uniqueness of boroughs based on all variables.

```{r}
#indata = lnd_boro_scaled %>% st_drop_geometry() %>% select(-name,-id)


#openxlsx::write.xlsx(lnd_boro_uniq %>% st_drop_geometry() %>% mutate_if(is.numeric, round, 4), 'analysis/london_boroughs_scaled_uniqueness.xlsx')

```

### Viz uniqueness 

```{r}
lnd_boro_uniq %>% select(uniq) %>% plot(border = 'white', nbreaks = 10, breaks='pretty')
```



## Countries (World Bank data)

```{r}

```

## EU (Eurostat data)


```{r}

```

End of notebook
